{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e165c6a0",
   "metadata": {
    "papermill": {
     "duration": 0.003941,
     "end_time": "2024-02-20T05:41:54.230446",
     "exception": false,
     "start_time": "2024-02-20T05:41:54.226505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee63ec25",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-20T05:41:54.240441Z",
     "iopub.status.busy": "2024-02-20T05:41:54.239279Z",
     "iopub.status.idle": "2024-02-20T05:41:56.632228Z",
     "shell.execute_reply": "2024-02-20T05:41:56.630992Z"
    },
    "papermill": {
     "duration": 2.400377,
     "end_time": "2024-02-20T05:41:56.634667",
     "exception": false,
     "start_time": "2024-02-20T05:41:54.234290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "# finds all the pathnames matching a specified pattern according to Unix shell rules\n",
    "from glob import glob\n",
    "# imports Python's garbage collector\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085bea3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T05:41:56.643623Z",
     "iopub.status.busy": "2024-02-20T05:41:56.643146Z",
     "iopub.status.idle": "2024-02-20T05:41:56.695795Z",
     "shell.execute_reply": "2024-02-20T05:41:56.694720Z"
    },
    "papermill": {
     "duration": 0.059801,
     "end_time": "2024-02-20T05:41:56.698134",
     "exception": false,
     "start_time": "2024-02-20T05:41:56.638333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 465 entries, 0 to 464\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Variable     465 non-null    object\n",
      " 1   Description  465 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actualdpd_943P</td>\n",
       "      <td>Days Past Due (DPD) of previous contract (actu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actualdpdtolerance_344P</td>\n",
       "      <td>DPD of client with tolerance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>addres_district_368M</td>\n",
       "      <td>District of the person's address.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>addres_role_871L</td>\n",
       "      <td>Role of person's address.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>addres_zip_823M</td>\n",
       "      <td>Zip code of the address.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Variable                                        Description\n",
       "0           actualdpd_943P  Days Past Due (DPD) of previous contract (actu...\n",
       "1  actualdpdtolerance_344P                      DPD of client with tolerance.\n",
       "2     addres_district_368M                  District of the person's address.\n",
       "3         addres_role_871L                          Role of person's address.\n",
       "4          addres_zip_823M                           Zip code of the address."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define a configuration class to hold directory paths\n",
    "class CFG:\n",
    "    # root directory where the dataset is stored\n",
    "    root_dir = '/kaggle/input/home-credit-credit-risk-model-stability'\n",
    "    # directory where training data files are located\n",
    "    train_dir = os.path.join(root_dir, 'parquet_files', 'train')\n",
    "    # directory where test data files are located\n",
    "    test_dir = os.path.join(root_dir, 'parquet_files', 'test')\n",
    "# load feature definitions and display\n",
    "feature_definitions_df = pd.read_csv(os.path.join(CFG.root_dir, \"feature_definitions.csv\"))\n",
    "# display the relative infomation of the dataset\n",
    "display(feature_definitions_df.info())\n",
    "display(feature_definitions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d37cf",
   "metadata": {
    "papermill": {
     "duration": 0.003629,
     "end_time": "2024-02-20T05:41:56.705889",
     "exception": false,
     "start_time": "2024-02-20T05:41:56.702260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8842d919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T05:41:56.715834Z",
     "iopub.status.busy": "2024-02-20T05:41:56.715141Z",
     "iopub.status.idle": "2024-02-20T05:41:56.727131Z",
     "shell.execute_reply": "2024-02-20T05:41:56.726105Z"
    },
    "papermill": {
     "duration": 0.019712,
     "end_time": "2024-02-20T05:41:56.729378",
     "exception": false,
     "start_time": "2024-02-20T05:41:56.709666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### define data process pipeline\n",
    "class Pipeline:\n",
    "    @staticmethod # avoid initializing a class instance\n",
    "    def set_table_dtypes(df):\n",
    "        # iterate over each column in the DataFrame\n",
    "        for col in df.columns:\n",
    "            # if column is in the specified list, cast it to 'Int64' \n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df[col] = df[col].astype('Int64')\n",
    "            # if column is 'date_decision', convert it to datetime\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "            # if column name ends with 'P' or 'A', cast it to 'float64'\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df[col] = df[col].astype('float64')\n",
    "            # if column name ends with 'M', cast it to 'string'\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df[col] = df[col].astype('string')\n",
    "            # if column name ends with 'D', convert it to datetime\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "                \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_dates(df):\n",
    "        # iterate over each column in the DataFrame\n",
    "        for col in df.columns:\n",
    "            # if column name ends with 'D', perform date subtraction and convert to days\n",
    "            if col[-1] in (\"D\",):\n",
    "                df[col] = (df[col] - df[\"date_decision\"]).dt.days\n",
    "\n",
    "        # drop the 'date_decision' and 'MONTH' columns\n",
    "        df = df.drop(columns=[\"date_decision\", \"MONTH\"])\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_cols(df):\n",
    "        # collect columns with more than 95% null values\n",
    "        cols_to_drop = [col for col in df.columns if col not in [\"target\", \"case_id\", \"WEEK_NUM\"] and df[col].isnull().mean() > 0.95]\n",
    "        # drop the identified columns from the DataFrame\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        # collect columns that are not 'target', 'case_id', or 'WEEK_NUM' and have a problematic number of unique values\n",
    "        cols_to_drop = [col for col in df.columns if col not in [\"target\", \"case_id\", \"WEEK_NUM\"] and df[col].dtype == object and (df[col].nunique() == 1 or df[col].nunique() > 200)]\n",
    "        # drop the identified columns from the DataFrame\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8464cd2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T05:41:56.739712Z",
     "iopub.status.busy": "2024-02-20T05:41:56.738629Z",
     "iopub.status.idle": "2024-02-20T05:41:56.748755Z",
     "shell.execute_reply": "2024-02-20T05:41:56.748015Z"
    },
    "papermill": {
     "duration": 0.017383,
     "end_time": "2024-02-20T05:41:56.750866",
     "exception": false,
     "start_time": "2024-02-20T05:41:56.733483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a class to perform aggregation operations on a DataFrame based on naming conventions of columns\n",
    "class Aggregator:\n",
    "    # define the list of aggregation functions for numerical columns\n",
    "    num_aggregators = ['max', 'min', 'mean']\n",
    "    # define the list of aggregation functions for string (categorical) columns\n",
    "    str_aggregators = ['max', 'min']\n",
    "    \n",
    "    # static method to generate a dictionary for aggregating numerical columns\n",
    "    @staticmethod\n",
    "    def num_expr(df):\n",
    "        # identify numerical columns by checking if the last letter of the column name is 'P' or 'A'\n",
    "        num_cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        # create a dictionary where column names map to the defined list of numeric aggregation functions\n",
    "        expr_all = {col: Aggregator.num_aggregators for col in num_cols}\n",
    "        return expr_all\n",
    "\n",
    "    @staticmethod\n",
    "    def date_expr(df):\n",
    "        # identify date columns by checking if the last letter of the column name is 'D'\n",
    "        date_cols = [col for col in df.columns if col[-1] in (\"D\",)]\n",
    "        # create a dictionary where column names map to the defined list of numeric aggregation functions\n",
    "        # assuming that the same numeric aggregations apply to date columns\n",
    "        expr_all = {col: Aggregator.num_aggregators for col in date_cols}\n",
    "        return expr_all\n",
    "\n",
    "    @staticmethod\n",
    "    def str_expr(df):\n",
    "        # identify string columns by checking if the last letter of the column name is 'M'\n",
    "        str_cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        # create a dictionary where column names map to the defined list of string aggregation functions\n",
    "        expr_all = {col: Aggregator.str_aggregators for col in str_cols}\n",
    "        return expr_all\n",
    "\n",
    "    @staticmethod\n",
    "    def get_exprs(df):\n",
    "        # merge dictionaries from numerical, date, and string expressions\n",
    "        exprs = {**Aggregator.num_expr(df),\n",
    "                 **Aggregator.date_expr(df),\n",
    "                 **Aggregator.str_expr(df)}\n",
    "        return exprs\n",
    "\n",
    "    @staticmethod\n",
    "    def aggregate(df, groupby_cols):\n",
    "        # get the combined dictionary of aggregation expressions\n",
    "        exprs = Aggregator.get_exprs(df)\n",
    "        # group the DataFrame by the specified columns and apply the aggregation expressions\n",
    "        aggregated_df = df.groupby(groupby_cols).agg(exprs)\n",
    "        # return the aggregated DataFrame\n",
    "        return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d044b5",
   "metadata": {
    "papermill": {
     "duration": 0.003584,
     "end_time": "2024-02-20T05:41:56.758413",
     "exception": false,
     "start_time": "2024-02-20T05:41:56.754829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5594314",
   "metadata": {
    "papermill": {
     "duration": 0.003575,
     "end_time": "2024-02-20T05:41:56.765758",
     "exception": false,
     "start_time": "2024-02-20T05:41:56.762183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f67da7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T05:41:56.775575Z",
     "iopub.status.busy": "2024-02-20T05:41:56.774860Z",
     "iopub.status.idle": "2024-02-20T05:41:56.781462Z",
     "shell.execute_reply": "2024-02-20T05:41:56.780776Z"
    },
    "papermill": {
     "duration": 0.013893,
     "end_time": "2024-02-20T05:41:56.783510",
     "exception": false,
     "start_time": "2024-02-20T05:41:56.769617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### perform feature engineering on the given dataset\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    # create new columns 'month_decision' and 'weekday_decision' by extracting\n",
    "    # the month and weekday from the 'date_decision' column.\n",
    "    df_base['month_decision'] = df_base['date_decision'].dt.month\n",
    "    df_base['weekday_decision'] = df_base['date_decision'].dt.weekday\n",
    "\n",
    "    # concatenate the lists of DataFrames for iteration\n",
    "    depth_frames = depth_0 + depth_1 + depth_2\n",
    "\n",
    "    # join each DataFrame in the concatenated list to df_base\n",
    "    for i, df in enumerate(depth_frames):\n",
    "        # create a suffix using the loop index to avoid column name collisions\n",
    "        suffix = f\"_{i}\"\n",
    "\n",
    "        # perform a left join with df_base on the 'case_id' column\n",
    "        df_base = df_base.merge(df, how=\"left\", on=\"case_id\", suffixes=('', suffix))\n",
    "\n",
    "    # process the dataset\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "\n",
    "    return df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56073dd",
   "metadata": {
    "papermill": {
     "duration": 0.003563,
     "end_time": "2024-02-20T05:41:56.791024",
     "exception": false,
     "start_time": "2024-02-20T05:41:56.787461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "738e4dfe",
   "metadata": {
    "papermill": {
     "duration": 0.003568,
     "end_time": "2024-02-20T05:41:56.798343",
     "exception": false,
     "start_time": "2024-02-20T05:41:56.794775",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare Dada Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17c29c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-20T05:41:56.807971Z",
     "iopub.status.busy": "2024-02-20T05:41:56.807249Z",
     "iopub.status.idle": "2024-02-20T05:41:56.814192Z",
     "shell.execute_reply": "2024-02-20T05:41:56.813406Z"
    },
    "papermill": {
     "duration": 0.014382,
     "end_time": "2024-02-20T05:41:56.816447",
     "exception": false,
     "start_time": "2024-02-20T05:41:56.802065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_df(data_dir, cat_cols=None, mode='train', display_store=False, train_cols=[]):\n",
    "    print('Collecting data...')\n",
    "    data_store = {\n",
    "        'df_base': read_file(data_dir / f'{mode}_base.parquet'),\n",
    "        'depth_0': [\n",
    "            read_file(data_dir / f'{mode}_static_cb_0.parquet'),\n",
    "            read_files(data_dir / f'{mode}_static_0_*.parquet')\n",
    "        ],\n",
    "        'depth_1': [\n",
    "            read_files(data_dir / f'{mode}_applprev_1_*.parquet', 1),\n",
    "            read_file(data_dir / f'{mode}_tax_registry_a_1.parquet', 1),\n",
    "            read_file(data_dir / f'{mode}_tax_registry_b_1.parquet', 1),\n",
    "            read_file(data_dir / f'{mode}_tax_registry_c_1.parquet', 1),\n",
    "            read_file(data_dir / f'{mode}_credit_bureau_b_1.parquet', 1),\n",
    "            read_file(data_dir / f'{mode}_other_1.parquet', 1),\n",
    "            read_file(data_dir / f'{mode}_person_1.parquet')\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bad773",
   "metadata": {
    "papermill": {
     "duration": 0.003583,
     "end_time": "2024-02-20T05:41:56.824086",
     "exception": false,
     "start_time": "2024-02-20T05:41:56.820503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7602123,
     "sourceId": 50160,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.899937,
   "end_time": "2024-02-20T05:41:57.449209",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-20T05:41:51.549272",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
